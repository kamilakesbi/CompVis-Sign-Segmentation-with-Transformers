{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.insert(1, os.path.split(os.getcwd())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataloader import DataLoader\n",
    "from model import Trainer\n",
    "from utils.utils import create_folders\n",
    "from batch_gen import BatchGenerator\n",
    "\n",
    "import torch \n",
    "\n",
    "from transformer import TransformerClassifier, TransfromerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(): \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.train_data = 'bslcp'\n",
    "        self.test_data = 'bslcp'\n",
    "        self.i3d_training = 'i3d_kinetics_bslcp_981'\n",
    "        self.num_in_frames = 16\n",
    "        self.features_dim = 1024\n",
    "        self.weights = 'opt'\n",
    "        self.regression = 0 \n",
    "        self.feature_normalization = 0\n",
    "        self.eval_use_CP = 0\n",
    "\n",
    "        self.action = 'train'\n",
    "        self.seed = 0 \n",
    "        self.refresh = 'store_true'\n",
    "\n",
    "        ## Transformer : \n",
    "        self.nhead = 4\n",
    "        self.nhid = 1024\n",
    "        self.dim_feedforward = 1024\n",
    "        self.num_layers = 6\n",
    "        self.dropout = 0\n",
    "\n",
    "        ## MSTCN : \n",
    "        self.num_stages = 4\n",
    "        self.num_layers = 10 \n",
    "        self.num_f_maps = 64\n",
    "        self.features_dim = 1024\n",
    "        self.bz = 8 \n",
    "        self.lr = 0.0005\n",
    "        self.lr_mul = 1\n",
    "        self.num_epochs = 50\n",
    "        self.extract_epoch = 10 \n",
    "        self.classification_threshold = 0.5\n",
    "        \n",
    "        ## Optimization\n",
    "        self.n_warmup_steps = 100\n",
    "        ## save model : \n",
    "        self.use_pseudo_labels = 'store_true'\n",
    "        self.pretrained = False\n",
    "        self.folder = ''\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train data: bslcp\n",
      "Load test data: bslcp\n"
     ]
    }
   ],
   "source": [
    "# load train dataset and test dataset\n",
    "\n",
    "print(f'Load train data: {args.train_data}')\n",
    "train_loader = DataLoader(args, args.train_data, 'train')\n",
    "print(f'Load test data: {args.test_data}')\n",
    "test_loader = DataLoader(args, args.test_data, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of class :  2\n",
      "cross entropy loss weigths :  [0.11247607877029446, 0.8875239212297056]\n",
      "number of videos in train :  5413\n",
      "number of videos in test :  702\n"
     ]
    }
   ],
   "source": [
    "## Some infos : \n",
    "print(\"number of class : \", train_loader.num_classes)\n",
    "print(\"cross entropy loss weigths : \", train_loader.weights)\n",
    "print('number of videos in train : ', len(train_loader.vid_list))\n",
    "print('number of videos in test : ', len(test_loader.vid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved options to ./exps//models/classification/traindata_bslcp/i3d_kinetics_bslcp_981/supervised/4_10_64_1024_8_0.0005_weighted_opt/seed_0/opt.txt\n",
      "./exps//models/classification/traindata_bslcp/i3d_kinetics_bslcp_981/supervised/4_10_64_1024_8_0.0005_weighted_opt/seed_0\n"
     ]
    }
   ],
   "source": [
    "model_load_dir, model_save_dir, results_save_dir = create_folders(args)\n",
    "print(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhid = 1024  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "dim_feedforward = 1024\n",
    "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # the number of heads in the multiheadattention models\n",
    "dropout = 0.1  # the dropout value\n",
    "\n",
    "nclasses = 2 # for classification task only\n",
    "\n",
    "model = TransformerClassifier(nhead, nhid, dim_feedforward, nlayers, nclasses, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TransfromerTrainer(\n",
    "    args.nhead,\n",
    "    args.nhid,\n",
    "    args.dim_feedforward,\n",
    "    args.num_layers,\n",
    "    nclasses,\n",
    "    args.dropout, \n",
    "    device,\n",
    "    train_loader.weights,\n",
    "    model_save_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = [\n",
    "    args,\n",
    "    model_save_dir,\n",
    "    results_save_dir,\n",
    "    test_loader.features_dict,\n",
    "    test_loader.gt_dict,\n",
    "    test_loader.eval_gt_dict,\n",
    "    test_loader.vid_list,\n",
    "    args.num_epochs,\n",
    "    device,\n",
    "    'eval',\n",
    "    args.classification_threshold,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = BatchGenerator(\n",
    "        train_loader.num_classes,\n",
    "        train_loader.gt_dict,\n",
    "        train_loader.features_dict,\n",
    "        train_loader.eval_gt_dict\n",
    "        )\n",
    "\n",
    "batch_gen.read_data(train_loader.vid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50/676.625) Batch: 0.3s | Total: 0:00:15 | ETA: 0:03:48 | LR: 0.0015625 | Loss: 0.7196210622787476\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:31 | ETA: 0:03:34 | LR: 0.003125 | Loss: 0.7217093110084534\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:48 | ETA: 0:03:40 | LR: 0.0025516 | Loss: 0.7106060981750488\n",
      "(200/676.625) Batch: 0.3s | Total: 0:01:04 | ETA: 0:03:41 | LR: 0.0022097 | Loss: 0.7090731859207153\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:21 | ETA: 0:03:29 | LR: 0.0019764 | Loss: 0.7054544687271118\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:37 | ETA: 0:03:17 | LR: 0.0018042 | Loss: 0.6983776688575745\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:53 | ETA: 0:02:55 | LR: 0.0016704 | Loss: 0.7040632367134094\n",
      "(400/676.625) Batch: 0.3s | Total: 0:02:09 | ETA: 0:03:17 | LR: 0.0015625 | Loss: 0.7003839015960693\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:26 | ETA: 0:03:20 | LR: 0.0014731 | Loss: 0.6980838179588318\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:43 | ETA: 0:03:53 | LR: 0.0013975 | Loss: 0.7006405591964722\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:59 | ETA: 0:03:36 | LR: 0.0013325 | Loss: 0.69875168800354\n",
      "(600/676.625) Batch: 0.3s | Total: 0:03:15 | ETA: 0:03:10 | LR: 0.0012758 | Loss: 0.6956532597541809\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:32 | ETA: 0:03:38 | LR: 0.0012257 | Loss: 0.7009261846542358\n",
      "epoch ok\n",
      "\n",
      "[E1 / train]: epoch loss = 0.7145,   acc = 51.18, mean F1B = 48.96, mean_F1S = 11.66\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:17 | LR: 0.001159 | Loss: 0.6985692977905273\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:29 | ETA: 0:03:33 | LR: 0.0011211 | Loss: 0.7005796432495117\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:46 | ETA: 0:03:14 | LR: 0.0010867 | Loss: 0.693604052066803\n",
      "(200/676.625) Batch: 0.3s | Total: 0:01:01 | ETA: 0:03:19 | LR: 0.0010552 | Loss: 0.6930859684944153\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:16 | ETA: 0:03:30 | LR: 0.0010264 | Loss: 0.699680507183075\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:32 | ETA: 0:03:15 | LR: 0.0009998 | Loss: 0.7004503011703491\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:48 | ETA: 0:03:30 | LR: 0.0009751 | Loss: 0.6924805641174316\n",
      "(400/676.625) Batch: 0.3s | Total: 0:02:04 | ETA: 0:03:04 | LR: 0.0009522 | Loss: 0.6980594992637634\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:19 | ETA: 0:03:29 | LR: 0.0009309 | Loss: 0.7030596137046814\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:34 | ETA: 0:03:10 | LR: 0.0009109 | Loss: 0.698523223400116\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:49 | ETA: 0:03:05 | LR: 0.0008921 | Loss: 0.6998591423034668\n",
      "(600/676.625) Batch: 0.3s | Total: 0:03:06 | ETA: 0:03:08 | LR: 0.0008745 | Loss: 0.6915256381034851\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:23 | ETA: 0:03:27 | LR: 0.0008579 | Loss: 0.6932640671730042\n",
      "epoch ok\n",
      "\n",
      "[E2 / train]: epoch loss = 0.6964,   acc = 38.94, mean F1B = 44.98, mean_F1S = 10.12\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:17 | ETA: 0:04:00 | LR: 0.000834 | Loss: 0.7021849155426025\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:32 | ETA: 0:03:42 | LR: 0.0008195 | Loss: 0.6944738626480103\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:47 | ETA: 0:03:17 | LR: 0.0008058 | Loss: 0.7024062275886536\n",
      "(200/676.625) Batch: 0.3s | Total: 0:01:04 | ETA: 0:03:27 | LR: 0.0007927 | Loss: 0.701092541217804\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:19 | ETA: 0:03:39 | LR: 0.0007803 | Loss: 0.6913045048713684\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:35 | ETA: 0:03:19 | LR: 0.0007684 | Loss: 0.6975705623626709\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:51 | ETA: 0:03:42 | LR: 0.000757 | Loss: 0.6934252977371216\n",
      "(400/676.625) Batch: 0.3s | Total: 0:02:06 | ETA: 0:03:25 | LR: 0.0007462 | Loss: 0.7000172138214111\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:23 | ETA: 0:03:49 | LR: 0.0007358 | Loss: 0.6899599432945251\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:39 | ETA: 0:03:11 | LR: 0.0007258 | Loss: 0.6964194774627686\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:55 | ETA: 0:03:39 | LR: 0.0007162 | Loss: 0.6931774020195007\n",
      "(600/676.625) Batch: 0.3s | Total: 0:03:11 | ETA: 0:02:48 | LR: 0.0007069 | Loss: 0.6906018257141113\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:26 | ETA: 0:02:55 | LR: 0.0006981 | Loss: 0.6902612447738647\n",
      "epoch ok\n",
      "\n",
      "[E3 / train]: epoch loss = 0.6956,   acc = 52.19, mean F1B = 47.42, mean_F1S = 10.93\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:13 | LR: 0.000685 | Loss: 0.6897607445716858\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:29 | ETA: 0:03:17 | LR: 0.000677 | Loss: 0.6954165101051331\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:43 | ETA: 0:03:35 | LR: 0.0006691 | Loss: 0.6985044479370117\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:58 | ETA: 0:03:14 | LR: 0.0006616 | Loss: 0.6933419704437256\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:12 | ETA: 0:02:51 | LR: 0.0006543 | Loss: 0.6980238556861877\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:26 | ETA: 0:02:57 | LR: 0.0006473 | Loss: 0.7012982964515686\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:40 | ETA: 0:02:55 | LR: 0.0006404 | Loss: 0.6912181973457336\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:54 | ETA: 0:03:16 | LR: 0.0006338 | Loss: 0.6936903595924377\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:09 | ETA: 0:02:54 | LR: 0.0006274 | Loss: 0.6961908340454102\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:23 | ETA: 0:02:52 | LR: 0.0006212 | Loss: 0.6892386078834534\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:38 | ETA: 0:03:16 | LR: 0.0006151 | Loss: 0.6988080143928528\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:52 | ETA: 0:02:43 | LR: 0.0006092 | Loss: 0.691469669342041\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:07 | ETA: 0:02:57 | LR: 0.0006035 | Loss: 0.6951842308044434\n",
      "epoch ok\n",
      "\n",
      "[E4 / train]: epoch loss = 0.6950,   acc = 11.48, mean F1B = 3.04, mean_F1S = 0.41\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:30 | LR: 0.000595 | Loss: 0.6970016956329346\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:28 | ETA: 0:03:03 | LR: 0.0005897 | Loss: 0.6997950673103333\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:43 | ETA: 0:03:24 | LR: 0.0005845 | Loss: 0.6942394375801086\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:57 | ETA: 0:02:54 | LR: 0.0005795 | Loss: 0.6953766345977783\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:11 | ETA: 0:03:13 | LR: 0.0005746 | Loss: 0.6915680766105652\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:26 | ETA: 0:03:04 | LR: 0.0005698 | Loss: 0.6921898126602173\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:40 | ETA: 0:03:13 | LR: 0.0005651 | Loss: 0.6885274052619934\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:55 | ETA: 0:03:09 | LR: 0.0005605 | Loss: 0.694665253162384\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:09 | ETA: 0:02:58 | LR: 0.0005561 | Loss: 0.6910216808319092\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:24 | ETA: 0:03:10 | LR: 0.0005517 | Loss: 0.6953092217445374\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:39 | ETA: 0:03:16 | LR: 0.0005475 | Loss: 0.6951335072517395\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:55 | ETA: 0:03:20 | LR: 0.0005433 | Loss: 0.6957881450653076\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:12 | ETA: 0:03:37 | LR: 0.0005393 | Loss: 0.6973392367362976\n",
      "epoch ok\n",
      "\n",
      "[E5 / train]: epoch loss = 0.6948,   acc = 80.13, mean F1B = 25.60, mean_F1S = 5.22\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:18 | LR: 0.0005332 | Loss: 0.6897855401039124\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:29 | ETA: 0:03:17 | LR: 0.0005294 | Loss: 0.694202184677124\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:44 | ETA: 0:03:22 | LR: 0.0005256 | Loss: 0.6896167397499084\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:58 | ETA: 0:02:59 | LR: 0.0005219 | Loss: 0.6952822804450989\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:12 | ETA: 0:03:01 | LR: 0.0005183 | Loss: 0.6934973001480103\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:28 | ETA: 0:03:41 | LR: 0.0005148 | Loss: 0.6962684392929077\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:44 | ETA: 0:03:15 | LR: 0.0005113 | Loss: 0.6953777074813843\n",
      "(400/676.625) Batch: 0.3s | Total: 0:02:01 | ETA: 0:03:22 | LR: 0.0005079 | Loss: 0.6961971521377563\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:17 | ETA: 0:03:05 | LR: 0.0005046 | Loss: 0.6948292851448059\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:33 | ETA: 0:03:27 | LR: 0.0005014 | Loss: 0.6953998804092407\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:49 | ETA: 0:03:21 | LR: 0.0004982 | Loss: 0.6912145614624023\n",
      "(600/676.625) Batch: 0.3s | Total: 0:03:05 | ETA: 0:02:51 | LR: 0.000495 | Loss: 0.6948728561401367\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:21 | ETA: 0:03:37 | LR: 0.000492 | Loss: 0.6940332055091858\n",
      "epoch ok\n",
      "\n",
      "[E6 / train]: epoch loss = 0.6946,   acc = 30.41, mean F1B = 42.58, mean_F1S = 10.15\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:18 | LR: 0.0004873 | Loss: 0.6964775919914246\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:28 | ETA: 0:02:59 | LR: 0.0004844 | Loss: 0.6963648796081543\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:42 | ETA: 0:02:51 | LR: 0.0004815 | Loss: 0.6907327771186829\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:57 | ETA: 0:03:16 | LR: 0.0004787 | Loss: 0.6918149590492249\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:11 | ETA: 0:03:11 | LR: 0.0004759 | Loss: 0.6961156725883484\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:26 | ETA: 0:03:18 | LR: 0.0004732 | Loss: 0.6960881948471069\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:41 | ETA: 0:03:19 | LR: 0.0004705 | Loss: 0.6964166164398193\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:54 | ETA: 0:02:54 | LR: 0.0004678 | Loss: 0.6959132552146912\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:11 | ETA: 0:03:25 | LR: 0.0004652 | Loss: 0.695275068283081\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:26 | ETA: 0:02:54 | LR: 0.0004627 | Loss: 0.6909492015838623\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:42 | ETA: 0:02:58 | LR: 0.0004602 | Loss: 0.6888639330863953\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:58 | ETA: 0:03:09 | LR: 0.0004577 | Loss: 0.6980218291282654\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:14 | ETA: 0:03:07 | LR: 0.0004552 | Loss: 0.6945280432701111\n",
      "epoch ok\n",
      "\n",
      "[E7 / train]: epoch loss = 0.6944,   acc = 21.09, mean F1B = 31.53, mean_F1S = 7.17\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:15 | ETA: 0:03:04 | LR: 0.0004516 | Loss: 0.6972089409828186\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:30 | ETA: 0:03:05 | LR: 0.0004492 | Loss: 0.695895791053772\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:45 | ETA: 0:03:17 | LR: 0.0004469 | Loss: 0.6947237849235535\n",
      "(200/676.625) Batch: 0.3s | Total: 0:01:01 | ETA: 0:03:36 | LR: 0.0004447 | Loss: 0.6919019222259521\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:17 | ETA: 0:04:04 | LR: 0.0004424 | Loss: 0.6910608410835266\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:34 | ETA: 0:04:06 | LR: 0.0004402 | Loss: 0.6908835172653198\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:50 | ETA: 0:03:32 | LR: 0.0004381 | Loss: 0.6912853121757507\n",
      "(400/676.625) Batch: 0.3s | Total: 0:02:07 | ETA: 0:03:28 | LR: 0.0004359 | Loss: 0.6922780871391296\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:24 | ETA: 0:03:41 | LR: 0.0004338 | Loss: 0.6917564868927002\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:40 | ETA: 0:03:20 | LR: 0.0004317 | Loss: 0.6946859955787659\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:56 | ETA: 0:03:32 | LR: 0.0004297 | Loss: 0.6964558959007263\n",
      "(600/676.625) Batch: 0.3s | Total: 0:03:13 | ETA: 0:03:00 | LR: 0.0004277 | Loss: 0.6928917765617371\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:28 | ETA: 0:02:58 | LR: 0.0004257 | Loss: 0.6943623423576355\n",
      "epoch ok\n",
      "\n",
      "[E8 / train]: epoch loss = 0.6944,   acc = 50.06, mean F1B = 47.81, mean_F1S = 12.51\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:02:58 | LR: 0.0004227 | Loss: 0.6924726366996765\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:29 | ETA: 0:03:20 | LR: 0.0004208 | Loss: 0.6947422623634338\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:43 | ETA: 0:03:20 | LR: 0.0004189 | Loss: 0.6870359778404236\n",
      "(200/676.625) Batch: 0.3s | Total: 0:01:01 | ETA: 0:04:26 | LR: 0.000417 | Loss: 0.6969140768051147\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:16 | ETA: 0:03:53 | LR: 0.0004152 | Loss: 0.6981155872344971\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:31 | ETA: 0:02:53 | LR: 0.0004133 | Loss: 0.6942354440689087\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:45 | ETA: 0:03:04 | LR: 0.0004115 | Loss: 0.6968881487846375\n",
      "(400/676.625) Batch: 0.3s | Total: 0:02:00 | ETA: 0:02:49 | LR: 0.0004098 | Loss: 0.6971467733383179\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:14 | ETA: 0:03:02 | LR: 0.000408 | Loss: 0.6906223297119141\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:31 | ETA: 0:03:11 | LR: 0.0004063 | Loss: 0.6928170919418335\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:47 | ETA: 0:02:47 | LR: 0.0004046 | Loss: 0.6942402720451355\n",
      "(600/676.625) Batch: 0.3s | Total: 0:03:02 | ETA: 0:03:09 | LR: 0.0004029 | Loss: 0.6962440013885498\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:18 | ETA: 0:03:34 | LR: 0.0004012 | Loss: 0.6936051845550537\n",
      "epoch ok\n",
      "\n",
      "[E9 / train]: epoch loss = 0.6942,   acc = 22.94, mean F1B = 39.16, mean_F1S = 10.13\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:27 | LR: 0.0003987 | Loss: 0.6943223476409912\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:29 | ETA: 0:03:18 | LR: 0.0003971 | Loss: 0.693982720375061\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:43 | ETA: 0:03:24 | LR: 0.0003955 | Loss: 0.6984804272651672\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:58 | ETA: 0:02:55 | LR: 0.0003939 | Loss: 0.6918033957481384\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:12 | ETA: 0:03:00 | LR: 0.0003924 | Loss: 0.6940297484397888\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:27 | ETA: 0:03:15 | LR: 0.0003908 | Loss: 0.696555495262146\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:41 | ETA: 0:03:10 | LR: 0.0003893 | Loss: 0.7012879848480225\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:56 | ETA: 0:03:11 | LR: 0.0003878 | Loss: 0.6915270686149597\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:11 | ETA: 0:02:59 | LR: 0.0003863 | Loss: 0.6928544640541077\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:26 | ETA: 0:03:17 | LR: 0.0003849 | Loss: 0.696517288684845\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:43 | ETA: 0:03:31 | LR: 0.0003834 | Loss: 0.6915643811225891\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:59 | ETA: 0:03:27 | LR: 0.000382 | Loss: 0.6936159729957581\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:14 | ETA: 0:02:45 | LR: 0.0003806 | Loss: 0.6959737539291382\n",
      "epoch ok\n",
      "\n",
      "[E10 / train]: epoch loss = 0.6942,   acc = 74.06, mean F1B = 30.68, mean_F1S = 7.05\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:02:57 | LR: 0.0003784 | Loss: 0.6911723017692566\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:28 | ETA: 0:03:24 | LR: 0.000377 | Loss: 0.6927957534790039\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:42 | ETA: 0:03:13 | LR: 0.0003757 | Loss: 0.6980260610580444\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:56 | ETA: 0:03:05 | LR: 0.0003743 | Loss: 0.6925837993621826\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:11 | ETA: 0:03:02 | LR: 0.000373 | Loss: 0.6966985464096069\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:26 | ETA: 0:02:59 | LR: 0.0003717 | Loss: 0.690832793712616\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:40 | ETA: 0:03:09 | LR: 0.0003703 | Loss: 0.6957679986953735\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:55 | ETA: 0:03:22 | LR: 0.0003691 | Loss: 0.6951960325241089\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:10 | ETA: 0:02:59 | LR: 0.0003678 | Loss: 0.6917182207107544\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:24 | ETA: 0:02:46 | LR: 0.0003665 | Loss: 0.6984175443649292\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:39 | ETA: 0:02:35 | LR: 0.0003653 | Loss: 0.6953365802764893\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:53 | ETA: 0:03:08 | LR: 0.000364 | Loss: 0.6926444172859192\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:09 | ETA: 0:02:59 | LR: 0.0003628 | Loss: 0.6933850049972534\n",
      "epoch ok\n",
      "\n",
      "[E11 / train]: epoch loss = 0.6941,   acc = 83.99, mean F1B = 23.04, mean_F1S = 3.97\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:38 | LR: 0.0003609 | Loss: 0.6946653127670288\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:28 | ETA: 0:02:56 | LR: 0.0003597 | Loss: 0.6930870413780212\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:42 | ETA: 0:02:55 | LR: 0.0003585 | Loss: 0.6934324502944946\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:56 | ETA: 0:03:28 | LR: 0.0003574 | Loss: 0.6916620135307312\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:11 | ETA: 0:03:06 | LR: 0.0003562 | Loss: 0.6965073943138123\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:26 | ETA: 0:02:58 | LR: 0.000355 | Loss: 0.6885503530502319\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:40 | ETA: 0:03:17 | LR: 0.0003539 | Loss: 0.6974993348121643\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:54 | ETA: 0:03:02 | LR: 0.0003528 | Loss: 0.6928560137748718\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:08 | ETA: 0:03:02 | LR: 0.0003517 | Loss: 0.696523129940033\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:22 | ETA: 0:02:53 | LR: 0.0003505 | Loss: 0.692284882068634\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:37 | ETA: 0:03:01 | LR: 0.0003495 | Loss: 0.6948778629302979\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:51 | ETA: 0:02:57 | LR: 0.0003484 | Loss: 0.6927348375320435\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:06 | ETA: 0:02:41 | LR: 0.0003473 | Loss: 0.6964267492294312\n",
      "epoch ok\n",
      "\n",
      "[E12 / train]: epoch loss = 0.6940,   acc = 88.71, mean F1B = 0.48, mean_F1S = 0.07\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:16 | ETA: 0:03:11 | LR: 0.0003456 | Loss: 0.6906649470329285\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:30 | ETA: 0:03:09 | LR: 0.0003446 | Loss: 0.695481538772583\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:44 | ETA: 0:03:05 | LR: 0.0003436 | Loss: 0.693322479724884\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:58 | ETA: 0:03:18 | LR: 0.0003425 | Loss: 0.6948445439338684\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:13 | ETA: 0:03:14 | LR: 0.0003415 | Loss: 0.6951438784599304\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:29 | ETA: 0:02:55 | LR: 0.0003405 | Loss: 0.6934248208999634\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:46 | ETA: 0:03:38 | LR: 0.0003395 | Loss: 0.6965402960777283\n",
      "(400/676.625) Batch: 0.3s | Total: 0:02:00 | ETA: 0:03:06 | LR: 0.0003385 | Loss: 0.6927194595336914\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:16 | ETA: 0:03:31 | LR: 0.0003375 | Loss: 0.6933403015136719\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:33 | ETA: 0:03:31 | LR: 0.0003365 | Loss: 0.6918094754219055\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:48 | ETA: 0:03:22 | LR: 0.0003355 | Loss: 0.692218005657196\n",
      "(600/676.625) Batch: 0.3s | Total: 0:03:05 | ETA: 0:03:11 | LR: 0.0003346 | Loss: 0.6931735277175903\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:19 | ETA: 0:02:53 | LR: 0.0003336 | Loss: 0.6931394338607788\n",
      "epoch ok\n",
      "\n",
      "[E13 / train]: epoch loss = 0.6940,   acc = 30.30, mean F1B = 38.74, mean_F1S = 9.06\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:15 | ETA: 0:03:09 | LR: 0.0003322 | Loss: 0.6918178796768188\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:30 | ETA: 0:03:26 | LR: 0.0003312 | Loss: 0.6913027167320251\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:45 | ETA: 0:03:09 | LR: 0.0003303 | Loss: 0.6953988075256348\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:59 | ETA: 0:02:49 | LR: 0.0003294 | Loss: 0.6926537156105042\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:13 | ETA: 0:02:58 | LR: 0.0003285 | Loss: 0.6901519298553467\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:28 | ETA: 0:02:53 | LR: 0.0003276 | Loss: 0.6933688521385193\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:43 | ETA: 0:03:07 | LR: 0.0003267 | Loss: 0.6929449439048767\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:58 | ETA: 0:03:00 | LR: 0.0003258 | Loss: 0.6951297521591187\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:13 | ETA: 0:03:22 | LR: 0.0003249 | Loss: 0.6930276155471802\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:30 | ETA: 0:03:19 | LR: 0.000324 | Loss: 0.6948384046554565\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:47 | ETA: 0:03:26 | LR: 0.0003232 | Loss: 0.6960905194282532\n",
      "(600/676.625) Batch: 0.3s | Total: 0:03:03 | ETA: 0:03:38 | LR: 0.0003223 | Loss: 0.6966062188148499\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:19 | ETA: 0:02:57 | LR: 0.0003214 | Loss: 0.6943982839584351\n",
      "epoch ok\n",
      "\n",
      "[E14 / train]: epoch loss = 0.6940,   acc = 68.72, mean F1B = 39.70, mean_F1S = 8.83\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:15 | LR: 0.0003201 | Loss: 0.6930175423622131\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:29 | ETA: 0:03:12 | LR: 0.0003193 | Loss: 0.6929405927658081\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:44 | ETA: 0:03:25 | LR: 0.0003185 | Loss: 0.6940967440605164\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:58 | ETA: 0:03:04 | LR: 0.0003177 | Loss: 0.696495771408081\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:13 | ETA: 0:03:22 | LR: 0.0003168 | Loss: 0.6920214891433716\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:27 | ETA: 0:03:10 | LR: 0.000316 | Loss: 0.6957680583000183\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:41 | ETA: 0:03:04 | LR: 0.0003152 | Loss: 0.6945537328720093\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:56 | ETA: 0:03:08 | LR: 0.0003144 | Loss: 0.6938897967338562\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:09 | ETA: 0:02:55 | LR: 0.0003136 | Loss: 0.6934773325920105\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:24 | ETA: 0:02:53 | LR: 0.0003128 | Loss: 0.6954461932182312\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:39 | ETA: 0:03:20 | LR: 0.0003121 | Loss: 0.6898282766342163\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:53 | ETA: 0:02:52 | LR: 0.0003113 | Loss: 0.6940992474555969\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:07 | ETA: 0:02:50 | LR: 0.0003105 | Loss: 0.6940774321556091\n",
      "epoch ok\n",
      "\n",
      "[E15 / train]: epoch loss = 0.6939,   acc = 88.67, mean F1B = 0.57, mean_F1S = 0.06\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:03:08 | LR: 0.0003093 | Loss: 0.6948898434638977\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:28 | ETA: 0:03:28 | LR: 0.0003086 | Loss: 0.6905623078346252\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:43 | ETA: 0:03:22 | LR: 0.0003078 | Loss: 0.6953846216201782\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:58 | ETA: 0:03:11 | LR: 0.0003071 | Loss: 0.6972320079803467\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:13 | ETA: 0:02:55 | LR: 0.0003064 | Loss: 0.6951817870140076\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:27 | ETA: 0:02:55 | LR: 0.0003056 | Loss: 0.6979203224182129\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:41 | ETA: 0:03:07 | LR: 0.0003049 | Loss: 0.6921486258506775\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:56 | ETA: 0:03:10 | LR: 0.0003042 | Loss: 0.6918131709098816\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:11 | ETA: 0:03:13 | LR: 0.0003035 | Loss: 0.6936450004577637\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:25 | ETA: 0:02:56 | LR: 0.0003027 | Loss: 0.6934782266616821\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:41 | ETA: 0:03:06 | LR: 0.000302 | Loss: 0.6946789026260376\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:58 | ETA: 0:03:03 | LR: 0.0003013 | Loss: 0.6966286301612854\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:14 | ETA: 0:03:15 | LR: 0.0003006 | Loss: 0.691990852355957\n",
      "epoch ok\n",
      "\n",
      "[E16 / train]: epoch loss = 0.6938,   acc = 78.63, mean F1B = 28.27, mean_F1S = 6.62\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:07 | LR: 0.0002996 | Loss: 0.6972665190696716\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:29 | ETA: 0:03:08 | LR: 0.0002989 | Loss: 0.692702054977417\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:44 | ETA: 0:03:21 | LR: 0.0002982 | Loss: 0.6936755776405334\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:58 | ETA: 0:03:08 | LR: 0.0002975 | Loss: 0.6953837871551514\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:13 | ETA: 0:02:57 | LR: 0.0002969 | Loss: 0.6941051483154297\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:28 | ETA: 0:02:59 | LR: 0.0002962 | Loss: 0.6930440068244934\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:42 | ETA: 0:03:09 | LR: 0.0002955 | Loss: 0.6944971084594727\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:56 | ETA: 0:02:48 | LR: 0.0002949 | Loss: 0.6951131820678711\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:11 | ETA: 0:03:06 | LR: 0.0002942 | Loss: 0.6960495710372925\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:25 | ETA: 0:02:55 | LR: 0.0002936 | Loss: 0.6955272555351257\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:40 | ETA: 0:03:13 | LR: 0.0002929 | Loss: 0.6927651166915894\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:54 | ETA: 0:02:59 | LR: 0.0002923 | Loss: 0.6928697228431702\n",
      "(650/676.625) Batch: 0.3s | Total: 0:03:07 | ETA: 0:02:52 | LR: 0.0002916 | Loss: 0.6926981210708618\n",
      "epoch ok\n",
      "\n",
      "[E17 / train]: epoch loss = 0.6938,   acc = 88.72, mean F1B = 0.24, mean_F1S = 0.04\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:03:00 | LR: 0.0002907 | Loss: 0.6935722827911377\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:27 | ETA: 0:03:09 | LR: 0.00029 | Loss: 0.6923342943191528\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    model_save_dir,\n",
    "    batch_gen,\n",
    "    args.num_epochs,\n",
    "    args.bz,\n",
    "    args.lr,\n",
    "    device,\n",
    "    eval_args,\n",
    "    args.lr_mul,\n",
    "    args.n_warmup_steps,\n",
    "    pretrained=model_load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "interpreter": {
   "hash": "8d95d6b7a3c173ab153c673ec402a2591ade3b55cc535db1e60390b48c1dd47f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
