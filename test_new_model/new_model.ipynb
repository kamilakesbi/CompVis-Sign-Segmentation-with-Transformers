{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.insert(1, os.path.split(os.getcwd())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataloader import DataLoader\n",
    "from model import Trainer\n",
    "from utils.utils import create_folders\n",
    "from batch_gen import BatchGenerator\n",
    "\n",
    "import torch \n",
    "\n",
    "from transformer import TransformerClassifier, TransfromerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(): \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.train_data = 'bslcp'\n",
    "        self.test_data = 'bslcp'\n",
    "        self.i3d_training = 'i3d_kinetics_bslcp_981'\n",
    "        self.num_in_frames = 16\n",
    "        self.features_dim = 1024\n",
    "        self.weights = 'opt'\n",
    "        self.regression = 0 \n",
    "        self.feature_normalization = 0\n",
    "        self.eval_use_CP = 0\n",
    "\n",
    "        self.action = 'train'\n",
    "        self.seed = 0 \n",
    "        self.refresh = 'store_true'\n",
    "\n",
    "        ## Transformer : \n",
    "        self.nhead = 4\n",
    "        self.nhid = 1024\n",
    "        self.dim_feedforward = 1024\n",
    "        self.num_layers = 6\n",
    "        self.dropout = 0\n",
    "\n",
    "        ## MSTCN : \n",
    "        self.num_stages = 4\n",
    "        self.num_layers = 10 \n",
    "        self.num_f_maps = 64\n",
    "        self.features_dim = 1024\n",
    "        self.bz = 8 \n",
    "        self.lr = 0.0005\n",
    "        self.lr_mul = 1\n",
    "        self.num_epochs = 50\n",
    "        self.extract_epoch = 10 \n",
    "        self.classification_threshold = 0.5\n",
    "        \n",
    "        ## Optimization\n",
    "        self.n_warmup_steps = 100\n",
    "        ## save model : \n",
    "        self.use_pseudo_labels = 'store_true'\n",
    "        self.pretrained = False\n",
    "        self.folder = ''\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train data: bslcp\n",
      "Load test data: bslcp\n"
     ]
    }
   ],
   "source": [
    "# load train dataset and test dataset\n",
    "\n",
    "print(f'Load train data: {args.train_data}')\n",
    "train_loader = DataLoader(args, args.train_data, 'train')\n",
    "print(f'Load test data: {args.test_data}')\n",
    "test_loader = DataLoader(args, args.test_data, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of class :  2\n",
      "cross entropy loss weigths :  [0.11247607877029446, 0.8875239212297056]\n",
      "number of videos in train :  5413\n",
      "number of videos in test :  702\n"
     ]
    }
   ],
   "source": [
    "## Some infos : \n",
    "print(\"number of class : \", train_loader.num_classes)\n",
    "print(\"cross entropy loss weigths : \", train_loader.weights)\n",
    "print('number of videos in train : ', len(train_loader.vid_list))\n",
    "print('number of videos in test : ', len(test_loader.vid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved options to ./exps//models/classification/traindata_bslcp/i3d_kinetics_bslcp_981/supervised/4_10_64_1024_8_0.0005_weighted_opt/seed_0/opt.txt\n",
      "./exps//models/classification/traindata_bslcp/i3d_kinetics_bslcp_981/supervised/4_10_64_1024_8_0.0005_weighted_opt/seed_0\n"
     ]
    }
   ],
   "source": [
    "model_load_dir, model_save_dir, results_save_dir = create_folders(args)\n",
    "print(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhid = 1024  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "dim_feedforward = 1024\n",
    "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # the number of heads in the multiheadattention models\n",
    "dropout = 0.1  # the dropout value\n",
    "\n",
    "nclasses = 2 # for classification task only\n",
    "\n",
    "model = TransformerClassifier(nhead, nhid, dim_feedforward, nlayers, nclasses, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TransfromerTrainer(\n",
    "    args.nhead,\n",
    "    args.nhid,\n",
    "    args.dim_feedforward,\n",
    "    args.num_layers,\n",
    "    nclasses,\n",
    "    args.dropout, \n",
    "    device,\n",
    "    train_loader.weights,\n",
    "    model_save_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = [\n",
    "    args,\n",
    "    model_save_dir,\n",
    "    results_save_dir,\n",
    "    test_loader.features_dict,\n",
    "    test_loader.gt_dict,\n",
    "    test_loader.eval_gt_dict,\n",
    "    test_loader.vid_list,\n",
    "    args.num_epochs,\n",
    "    device,\n",
    "    'eval',\n",
    "    args.classification_threshold,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = BatchGenerator(\n",
    "        train_loader.num_classes,\n",
    "        train_loader.gt_dict,\n",
    "        train_loader.features_dict,\n",
    "        train_loader.eval_gt_dict\n",
    "        )\n",
    "\n",
    "batch_gen.read_data(train_loader.vid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:02:56 | LR: 0.0015625 | Loss: 1.0404775142669678\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:27 | ETA: 0:03:07 | LR: 0.003125 | Loss: 0.7026453614234924\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:40 | ETA: 0:02:54 | LR: 0.0025516 | Loss: 0.6971063017845154\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:54 | ETA: 0:02:43 | LR: 0.0022097 | Loss: 0.7319566011428833\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:07 | ETA: 0:02:50 | LR: 0.0019764 | Loss: 0.6718668937683105\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:21 | ETA: 0:03:02 | LR: 0.0018042 | Loss: 0.7014293074607849\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:34 | ETA: 0:02:41 | LR: 0.0016704 | Loss: 0.733392596244812\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:47 | ETA: 0:02:44 | LR: 0.0015625 | Loss: 0.6876875758171082\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:01 | ETA: 0:02:56 | LR: 0.0014731 | Loss: 0.6889294385910034\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:14 | ETA: 0:02:45 | LR: 0.0013975 | Loss: 0.7016133069992065\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:28 | ETA: 0:02:44 | LR: 0.0013325 | Loss: 0.6908870339393616\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:41 | ETA: 0:02:39 | LR: 0.0012758 | Loss: 0.7063942551612854\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:55 | ETA: 0:02:37 | LR: 0.0012257 | Loss: 0.7018201947212219\n",
      "epoch ok\n",
      "\n",
      "[E1 / train]: epoch loss = 0.7198,   acc = 54.23, mean F1B = 22.69, mean_F1S = 2.66\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:14 | ETA: 0:03:04 | LR: 0.001159 | Loss: 0.6917638778686523\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:27 | ETA: 0:03:15 | LR: 0.0011211 | Loss: 0.6924155950546265\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:40 | ETA: 0:03:08 | LR: 0.0010867 | Loss: 0.6973133683204651\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:54 | ETA: 0:03:06 | LR: 0.0010552 | Loss: 0.6915095448493958\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:07 | ETA: 0:02:40 | LR: 0.0010264 | Loss: 0.6949611902236938\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:20 | ETA: 0:02:54 | LR: 0.0009998 | Loss: 0.6952270865440369\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:34 | ETA: 0:02:39 | LR: 0.0009751 | Loss: 0.6941422820091248\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:47 | ETA: 0:03:05 | LR: 0.0009522 | Loss: 0.69951331615448\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:01 | ETA: 0:02:47 | LR: 0.0009309 | Loss: 0.6936551332473755\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:14 | ETA: 0:02:45 | LR: 0.0009109 | Loss: 0.6926478147506714\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:28 | ETA: 0:02:52 | LR: 0.0008921 | Loss: 0.688883364200592\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:41 | ETA: 0:02:51 | LR: 0.0008745 | Loss: 0.6939811706542969\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:55 | ETA: 0:02:43 | LR: 0.0008579 | Loss: 0.6966359615325928\n",
      "epoch ok\n",
      "\n",
      "[E2 / train]: epoch loss = 0.6954,   acc = 50.71, mean F1B = 27.94, mean_F1S = 2.92\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:12 | ETA: 0:03:07 | LR: 0.000834 | Loss: 0.6918538808822632\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:26 | ETA: 0:02:54 | LR: 0.0008195 | Loss: 0.6964784860610962\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:40 | ETA: 0:02:54 | LR: 0.0008058 | Loss: 0.6953638792037964\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:53 | ETA: 0:02:43 | LR: 0.0007927 | Loss: 0.6878834962844849\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:06 | ETA: 0:02:55 | LR: 0.0007803 | Loss: 0.6983888149261475\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:19 | ETA: 0:02:41 | LR: 0.0007684 | Loss: 0.6977039575576782\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:32 | ETA: 0:03:00 | LR: 0.000757 | Loss: 0.6886630058288574\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:45 | ETA: 0:02:53 | LR: 0.0007462 | Loss: 0.693253219127655\n",
      "(450/676.625) Batch: 0.3s | Total: 0:01:59 | ETA: 0:02:27 | LR: 0.0007358 | Loss: 0.6911453008651733\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:13 | ETA: 0:02:43 | LR: 0.0007258 | Loss: 0.6940292716026306\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:26 | ETA: 0:02:26 | LR: 0.0007162 | Loss: 0.6938838958740234\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:40 | ETA: 0:02:30 | LR: 0.0007069 | Loss: 0.696567952632904\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:54 | ETA: 0:02:43 | LR: 0.0006981 | Loss: 0.6900057196617126\n",
      "epoch ok\n",
      "\n",
      "[E3 / train]: epoch loss = 0.6946,   acc = 63.49, mean F1B = 22.71, mean_F1S = 2.85\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:03:14 | LR: 0.000685 | Loss: 0.6942796111106873\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:27 | ETA: 0:02:50 | LR: 0.000677 | Loss: 0.6953474879264832\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:40 | ETA: 0:03:11 | LR: 0.0006691 | Loss: 0.6911260485649109\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:54 | ETA: 0:02:57 | LR: 0.0006616 | Loss: 0.6935765147209167\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:07 | ETA: 0:03:03 | LR: 0.0006543 | Loss: 0.6961093544960022\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:20 | ETA: 0:02:42 | LR: 0.0006473 | Loss: 0.6905089616775513\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:33 | ETA: 0:02:51 | LR: 0.0006404 | Loss: 0.6955426931381226\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:46 | ETA: 0:02:34 | LR: 0.0006338 | Loss: 0.6987112164497375\n",
      "(450/676.625) Batch: 0.3s | Total: 0:01:59 | ETA: 0:02:32 | LR: 0.0006274 | Loss: 0.6956278681755066\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:13 | ETA: 0:02:51 | LR: 0.0006212 | Loss: 0.6953697800636292\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:27 | ETA: 0:02:48 | LR: 0.0006151 | Loss: 0.6883456707000732\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:40 | ETA: 0:03:00 | LR: 0.0006092 | Loss: 0.6901558637619019\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:54 | ETA: 0:02:36 | LR: 0.0006035 | Loss: 0.6966471076011658\n",
      "epoch ok\n",
      "\n",
      "[E4 / train]: epoch loss = 0.6943,   acc = 69.02, mean F1B = 22.91, mean_F1S = 2.87\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:02:53 | LR: 0.000595 | Loss: 0.6925628781318665\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:26 | ETA: 0:02:47 | LR: 0.0005897 | Loss: 0.6889028549194336\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:40 | ETA: 0:02:57 | LR: 0.0005845 | Loss: 0.6910584568977356\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:54 | ETA: 0:03:03 | LR: 0.0005795 | Loss: 0.6942422986030579\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:07 | ETA: 0:02:57 | LR: 0.0005746 | Loss: 0.6929138898849487\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:21 | ETA: 0:02:55 | LR: 0.0005698 | Loss: 0.696432888507843\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:33 | ETA: 0:02:32 | LR: 0.0005651 | Loss: 0.6896799802780151\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:46 | ETA: 0:02:51 | LR: 0.0005605 | Loss: 0.693541407585144\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:00 | ETA: 0:02:55 | LR: 0.0005561 | Loss: 0.6901981234550476\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:14 | ETA: 0:02:49 | LR: 0.0005517 | Loss: 0.695164680480957\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:27 | ETA: 0:02:32 | LR: 0.0005475 | Loss: 0.6977452635765076\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:41 | ETA: 0:02:39 | LR: 0.0005433 | Loss: 0.6980785131454468\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:54 | ETA: 0:02:35 | LR: 0.0005393 | Loss: 0.6949651837348938\n",
      "epoch ok\n",
      "\n",
      "[E5 / train]: epoch loss = 0.6939,   acc = 75.37, mean F1B = 18.77, mean_F1S = 3.21\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:02:54 | LR: 0.0005332 | Loss: 0.6951460242271423\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:27 | ETA: 0:03:12 | LR: 0.0005294 | Loss: 0.69652259349823\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:40 | ETA: 0:02:59 | LR: 0.0005256 | Loss: 0.6925844550132751\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:53 | ETA: 0:03:02 | LR: 0.0005219 | Loss: 0.6960026025772095\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:06 | ETA: 0:02:55 | LR: 0.0005183 | Loss: 0.6958454251289368\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:20 | ETA: 0:02:57 | LR: 0.0005148 | Loss: 0.691271960735321\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:33 | ETA: 0:02:34 | LR: 0.0005113 | Loss: 0.6937849521636963\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:47 | ETA: 0:02:54 | LR: 0.0005079 | Loss: 0.6921061873435974\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:00 | ETA: 0:02:46 | LR: 0.0005046 | Loss: 0.692115306854248\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:14 | ETA: 0:02:32 | LR: 0.0005014 | Loss: 0.6914855241775513\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:27 | ETA: 0:02:43 | LR: 0.0004982 | Loss: 0.6902763247489929\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:40 | ETA: 0:02:34 | LR: 0.000495 | Loss: 0.6940671801567078\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:54 | ETA: 0:02:53 | LR: 0.000492 | Loss: 0.6902958154678345\n",
      "epoch ok\n",
      "\n",
      "[E6 / train]: epoch loss = 0.6941,   acc = 71.05, mean F1B = 21.91, mean_F1S = 2.66\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:02:50 | LR: 0.0004873 | Loss: 0.6962944269180298\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:26 | ETA: 0:03:01 | LR: 0.0004844 | Loss: 0.6957312226295471\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:39 | ETA: 0:02:42 | LR: 0.0004815 | Loss: 0.6907558441162109\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:52 | ETA: 0:02:43 | LR: 0.0004787 | Loss: 0.6949822306632996\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:06 | ETA: 0:02:39 | LR: 0.0004759 | Loss: 0.6947521567344666\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:19 | ETA: 0:02:44 | LR: 0.0004732 | Loss: 0.691368043422699\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:33 | ETA: 0:03:05 | LR: 0.0004705 | Loss: 0.6942442059516907\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:47 | ETA: 0:02:57 | LR: 0.0004678 | Loss: 0.694397509098053\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:00 | ETA: 0:03:05 | LR: 0.0004652 | Loss: 0.696117639541626\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:13 | ETA: 0:02:36 | LR: 0.0004627 | Loss: 0.6968207955360413\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:27 | ETA: 0:02:53 | LR: 0.0004602 | Loss: 0.6942785978317261\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:41 | ETA: 0:02:50 | LR: 0.0004577 | Loss: 0.6902335286140442\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:54 | ETA: 0:02:41 | LR: 0.0004552 | Loss: 0.6944182515144348\n",
      "epoch ok\n",
      "\n",
      "[E7 / train]: epoch loss = 0.6941,   acc = 69.90, mean F1B = 21.91, mean_F1S = 3.46\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:12 | ETA: 0:03:01 | LR: 0.0004516 | Loss: 0.6926931738853455\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:26 | ETA: 0:02:48 | LR: 0.0004492 | Loss: 0.6914297342300415\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:40 | ETA: 0:03:05 | LR: 0.0004469 | Loss: 0.6999489665031433\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:53 | ETA: 0:02:57 | LR: 0.0004447 | Loss: 0.6925979256629944\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:07 | ETA: 0:02:57 | LR: 0.0004424 | Loss: 0.6933102607727051\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:20 | ETA: 0:02:58 | LR: 0.0004402 | Loss: 0.6952436566352844\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:33 | ETA: 0:02:57 | LR: 0.0004381 | Loss: 0.6904894113540649\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:46 | ETA: 0:03:01 | LR: 0.0004359 | Loss: 0.6934137940406799\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:00 | ETA: 0:02:46 | LR: 0.0004338 | Loss: 0.6887354850769043\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:14 | ETA: 0:02:25 | LR: 0.0004317 | Loss: 0.6936663389205933\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:27 | ETA: 0:02:39 | LR: 0.0004297 | Loss: 0.6937744617462158\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:41 | ETA: 0:02:49 | LR: 0.0004277 | Loss: 0.692596435546875\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:54 | ETA: 0:02:32 | LR: 0.0004257 | Loss: 0.6930092573165894\n",
      "epoch ok\n",
      "\n",
      "[E8 / train]: epoch loss = 0.6939,   acc = 68.55, mean F1B = 24.76, mean_F1S = 3.38\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:02:48 | LR: 0.0004227 | Loss: 0.6933478116989136\n",
      "(100/676.625) Batch: 0.3s | Total: 0:00:25 | ETA: 0:02:35 | LR: 0.0004208 | Loss: 0.6948977112770081\n",
      "(150/676.625) Batch: 0.3s | Total: 0:00:39 | ETA: 0:02:47 | LR: 0.0004189 | Loss: 0.6987966299057007\n",
      "(200/676.625) Batch: 0.3s | Total: 0:00:52 | ETA: 0:02:46 | LR: 0.000417 | Loss: 0.6909881234169006\n",
      "(250/676.625) Batch: 0.3s | Total: 0:01:06 | ETA: 0:02:55 | LR: 0.0004152 | Loss: 0.6974842548370361\n",
      "(300/676.625) Batch: 0.3s | Total: 0:01:19 | ETA: 0:02:45 | LR: 0.0004133 | Loss: 0.6924160122871399\n",
      "(350/676.625) Batch: 0.3s | Total: 0:01:32 | ETA: 0:03:03 | LR: 0.0004115 | Loss: 0.6956020593643188\n",
      "(400/676.625) Batch: 0.3s | Total: 0:01:46 | ETA: 0:02:44 | LR: 0.0004098 | Loss: 0.6928141713142395\n",
      "(450/676.625) Batch: 0.3s | Total: 0:02:00 | ETA: 0:02:46 | LR: 0.000408 | Loss: 0.6893757581710815\n",
      "(500/676.625) Batch: 0.3s | Total: 0:02:13 | ETA: 0:02:46 | LR: 0.0004063 | Loss: 0.6902072429656982\n",
      "(550/676.625) Batch: 0.3s | Total: 0:02:26 | ETA: 0:02:16 | LR: 0.0004046 | Loss: 0.6934800744056702\n",
      "(600/676.625) Batch: 0.3s | Total: 0:02:40 | ETA: 0:02:45 | LR: 0.0004029 | Loss: 0.6941376328468323\n",
      "(650/676.625) Batch: 0.3s | Total: 0:02:53 | ETA: 0:02:32 | LR: 0.0004012 | Loss: 0.6908509135246277\n",
      "epoch ok\n",
      "\n",
      "[E9 / train]: epoch loss = 0.6939,   acc = 64.54, mean F1B = 25.85, mean_F1S = 3.76\n",
      "(50/676.625) Batch: 0.3s | Total: 0:00:13 | ETA: 0:02:57 | LR: 0.0003987 | Loss: 0.691394031047821\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3616/2059999620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_mul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     pretrained=model_load_dir)\n\u001b[0m",
      "\u001b[0;32m~/MVARecVisProject/transformer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, save_dir, batch_gen, num_epochs, batch_size, learning_rate, device, eval_args, lr_mul, n_warmup_steps, pretrained)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;31m#optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_and_update_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    model_save_dir,\n",
    "    batch_gen,\n",
    "    args.num_epochs,\n",
    "    args.bz,\n",
    "    args.lr,\n",
    "    device,\n",
    "    eval_args,\n",
    "    args.lr_mul,\n",
    "    args.n_warmup_steps,\n",
    "    pretrained=model_load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "interpreter": {
   "hash": "8d95d6b7a3c173ab153c673ec402a2591ade3b55cc535db1e60390b48c1dd47f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
