{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.insert(1, os.path.split(os.getcwd())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataloader import DataLoader\n",
    "from model import Trainer\n",
    "from utils.utils import create_folders\n",
    "from batch_gen import BatchGenerator\n",
    "\n",
    "import torch \n",
    "\n",
    "from transformer import TransformerClassifier, TransfromerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(): \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.train_data = 'bslcp'\n",
    "        self.test_data = 'bslcp'\n",
    "        self.i3d_training = 'i3d_kinetics_bslcp_981'\n",
    "        self.num_in_frames = 16\n",
    "        self.features_dim = 1024\n",
    "        self.weights = 'opt'\n",
    "        self.regression = 0 \n",
    "        self.feature_normalization = 0\n",
    "        self.eval_use_CP = 0\n",
    "\n",
    "        self.action = 'train'\n",
    "        self.seed = 0 \n",
    "        self.refresh = 'store_true'\n",
    "\n",
    "        ## Transformer : \n",
    "        self.nhead = 4\n",
    "        self.nhid = 256\n",
    "        self.dim_feedforward = 1024\n",
    "        self.num_layers = 6\n",
    "        self.dropout = 0\n",
    "\n",
    "        ## MSTCN : \n",
    "        self.num_stages = 4\n",
    "        self.num_layers = 10 \n",
    "        self.num_f_maps = 64\n",
    "        self.features_dim = 1024\n",
    "        self.bz = 8 \n",
    "        self.lr = 0.0005\n",
    "        self.lr_mul = 1\n",
    "        self.num_epochs = 50\n",
    "        self.extract_epoch = 10 \n",
    "        self.classification_threshold = 0.5\n",
    "        \n",
    "        ## Optimization\n",
    "        self.n_warmup_steps = 100\n",
    "        ## save model : \n",
    "        self.use_pseudo_labels = 'store_true'\n",
    "        self.pretrained = False\n",
    "        self.folder = ''\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train data: bslcp\n",
      "Load test data: bslcp\n"
     ]
    }
   ],
   "source": [
    "# load train dataset and test dataset\n",
    "\n",
    "print(f'Load train data: {args.train_data}')\n",
    "train_loader = DataLoader(args, args.train_data, 'train')\n",
    "print(f'Load test data: {args.test_data}')\n",
    "test_loader = DataLoader(args, args.test_data, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of class :  2\n",
      "cross entropy loss weigths :  [0.11247607877029446, 0.8875239212297056]\n",
      "number of videos in train :  5413\n",
      "number of videos in test :  702\n"
     ]
    }
   ],
   "source": [
    "## Some infos : \n",
    "print(\"number of class : \", train_loader.num_classes)\n",
    "print(\"cross entropy loss weigths : \", train_loader.weights)\n",
    "print('number of videos in train : ', len(train_loader.vid_list))\n",
    "print('number of videos in test : ', len(test_loader.vid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved options to ./exps//models/classification/traindata_bslcp/i3d_kinetics_bslcp_981/supervised/(4,)_(10,)_(256,)_8_0.0005_weighted_opt/seed_0\\opt.txt\n",
      "./exps//models/classification/traindata_bslcp/i3d_kinetics_bslcp_981/supervised/(4,)_(10,)_(256,)_8_0.0005_weighted_opt/seed_0\n"
     ]
    }
   ],
   "source": [
    "model_load_dir, model_save_dir, results_save_dir = create_folders(args, model_type = 'transformer')\n",
    "print(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhid = 1024  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "dim_feedforward = 1024\n",
    "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # the number of heads in the multiheadattention models\n",
    "dropout = 0.1  # the dropout value\n",
    "\n",
    "nclasses = 2 # for classification task only\n",
    "\n",
    "model = TransformerClassifier(nhead, nhid, dim_feedforward, nlayers, nclasses, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kam_a\\.conda\\envs\\signseg_env\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\kam_a\\.conda\\envs\\signseg_env\\lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py\", line 238, in run\n",
      "    self._record_writer.write(data)\n",
      "  File \"C:\\Users\\kam_a\\.conda\\envs\\signseg_env\\lib\\site-packages\\tensorboard\\summary\\writer\\record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"C:\\Users\\kam_a\\.conda\\envs\\signseg_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 531, in write\n",
      "    self.fs.write(self.filename, file_content, self.binary_mode)\n",
      "  File \"C:\\Users\\kam_a\\.conda\\envs\\signseg_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 148, in write\n",
      "    self._write(filename, file_content, \"wb\" if binary_mode else \"w\")\n",
      "  File \"C:\\Users\\kam_a\\.conda\\envs\\signseg_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 162, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: b'./exps//models/classification/traindata_bslcp/i3d_kinetics_bslcp_981/supervised/(4,)_(10,)_(256,)_8_0.0005_weighted_opt/seed_0/logs\\\\events.out.tfevents.1642871786.DESKTOP-M7GVU52.2548.1'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = TransfromerTrainer(\n",
    "    args.nhead,\n",
    "    args.nhid,\n",
    "    args.dim_feedforward,\n",
    "    args.num_layers,\n",
    "    nclasses,\n",
    "    args.dropout, \n",
    "    device,\n",
    "    train_loader.weights,\n",
    "    model_save_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = [\n",
    "    args,\n",
    "    model_save_dir,\n",
    "    results_save_dir,\n",
    "    test_loader.features_dict,\n",
    "    test_loader.gt_dict,\n",
    "    test_loader.eval_gt_dict,\n",
    "    test_loader.vid_list,\n",
    "    args.num_epochs,\n",
    "    device,\n",
    "    'eval',\n",
    "    args.classification_threshold,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = BatchGenerator(\n",
    "        train_loader.num_classes,\n",
    "        train_loader.gt_dict,\n",
    "        train_loader.features_dict,\n",
    "        train_loader.eval_gt_dict\n",
    "        )\n",
    "\n",
    "batch_gen.read_data(train_loader.vid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50/676.625) Batch: 1.1s | Total: 0:00:53 | ETA: 0:10:20 | Loss: 0.6881963610649109\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2548/2059999620.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_mul\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     pretrained=model_load_dir)\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\kam_a\\OneDrive\\Bureau\\MVA\\RecVis\\RecVisProject\\MVARecVisProject\\transformer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, save_dir, batch_gen, num_epochs, batch_size, learning_rate, device, eval_args, lr_mul, n_warmup_steps, pretrained)\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m0.15\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;31m#optimizer.step_and_update_lr()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\signseg_env\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\signseg_env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    model_save_dir,\n",
    "    batch_gen,\n",
    "    args.num_epochs,\n",
    "    args.bz,\n",
    "    args.lr,\n",
    "    device,\n",
    "    eval_args,\n",
    "    args.lr_mul,\n",
    "    args.n_warmup_steps,\n",
    "    pretrained=model_load_dir)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "interpreter": {
   "hash": "8d95d6b7a3c173ab153c673ec402a2591ade3b55cc535db1e60390b48c1dd47f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
